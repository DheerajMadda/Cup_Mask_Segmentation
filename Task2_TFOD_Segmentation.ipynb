{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83938209",
   "metadata": {},
   "source": [
    "# Download the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a72832",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    !pip install CocoDataset==0.1.2\n",
    "    !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "    !unzip annotations_trainval2017.zip\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db13c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coco_dataset import coco_dataset_download as cocod\n",
    "class_name='cup'  \n",
    "images_count=50       \n",
    "annotations_path='.annotations_trainval2017/annotations/instances_val2017.json' \n",
    "cocod.coco_dataset_download(class_name,images_count,annotations_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f952515",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Dataset used: 50 random samples from COCO Cup Dataset\n",
    "    Data Annotation: Manual annotataion using Labelme tool (!pip install labelme)   \n",
    "    \n",
    "    Data Stored Path {\n",
    "        Train data         -> models/research/object_detection/traindata/training\n",
    "        Train annotation   -> models/research/object_detection/traindata/trainjson\n",
    "        Test data          -> models/research/object_detection/traindata/training\n",
    "        Test annotation    -> models/research/object_detection/traindata/testing\n",
    "        Lable map          -> models/research/object_detection/data/cup_label_map.pbtxt\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    !python models/research/create_tf_records.py\n",
    "                       -> OUTPUT: models/research/object_detection/data/cup_train.record\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bdc710",
   "metadata": {},
   "source": [
    "# Download MaskRCNN model form tensorflow model zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Download http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz \n",
    "    and extract it to -> models/research/pre-trained-models\n",
    "\n",
    "Source link (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9fe0e2",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c74cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    1) Edit the file -> cup_maskrcnn_model/mask_rcnn_inception_v2_coco.config accordingly\n",
    "        line numbers = 10, 133, 142, 144, 158, 160\n",
    "    \n",
    "    2) python models/research/train.py \\\n",
    "        --logtostderr \\\n",
    "        --train_dir=models/research/cup_maskrcnn_model_checkpoint/ \\\n",
    "        --pipeline_config_path=models/research/cup_maskrcnn_model_checkpoint/mask_rcnn_inception_v2_coco.config\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675857b0",
   "metadata": {},
   "source": [
    "# Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bceb12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    python models/research/export_inference_graph.py --input_type image_tensor \\\n",
    "        --pipeline_config_path models/research/cup_maskrcnn_model_checkpoint/mask_rcnn_inception_v2_coco.config \\\n",
    "        --trained_checkpoint_prefix models/research/cup_maskrcnn_model_checkpoint/model.ckpt-1138 \\\n",
    "        --output_directory models/research/cup_maskrcnn_model_export\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068aa1f8",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5faf220",
   "metadata": {},
   "source": [
    "# Import the libraries for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47228bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3581608",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b04518f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae3e85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_FROZEN_GRAPH = os.path.join(os.getcwd(),\n",
    "                                 \"models\",\n",
    "                                 \"research\",\n",
    "                                 \"cup_maskrcnn_model_export\",\n",
    "                                 \"frozen_inference_graph.pb\")\n",
    "\n",
    "PATH_TO_LABELS = os.path.join(os.getcwd(),\n",
    "                              \"models\", \n",
    "                              \"research\", \n",
    "                              \"object_detection\",\n",
    "                              \"data\",\n",
    "                              \"cup_label_map.pbtxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cbd2a7",
   "metadata": {},
   "source": [
    "## Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d74e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e449f7",
   "metadata": {},
   "source": [
    "## Loading label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80eaf7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\tf1\\lib\\site-packages\\object_detection\\utils\\label_map_util.py:132: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa18666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'id': 1, 'name': 'cup'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d048e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "        # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in [\n",
    "                'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                'detection_classes', 'detection_masks'\n",
    "            ]:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks_reframed = tf.cast(tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(detection_masks_reframed, 0)\n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Run inference\n",
    "            output_dict = sess.run(tensor_dict,feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865ab98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TEST_IMAGES_DIR = os.path.join(os.getcwd(), \"sample_test_images\")\n",
    "# for a single input file, uncomment below line and comment the above line\n",
    "# PATH_TO_TEST_IMAGES_DIR = [os.path.join(os.getcwd(), \"<Path to input image file(extension)>\")]\n",
    "TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR, i) for i in os.listdir(PATH_TO_TEST_IMAGES_DIR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604c2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in TEST_IMAGE_PATHS:\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_expanded = np.expand_dims(image, axis=0)\n",
    "\n",
    "    \n",
    "    output_dict = run_inference_for_single_image(image, detection_graph)\n",
    "    \n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks'),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=8)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
